{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc8ff9b-eb5d-4a4c-bcd1-1315c463b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 02:43:34.420021: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-23 02:43:34.469613: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-23 02:43:34.469640: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-23 02:43:34.470959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-23 02:43:34.478589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-23 02:43:35.443019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/vincent/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "from official.nlp import optimization\n",
    "from jcarbon.tensorflow.callbacks import JCarbonEpochCallback, JCarbonBatchCallback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import time\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f77a92-b8fa-49ed-81e0-8508a3366c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#add bert model\n",
    "#bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5dd3a56-dbd4-40a3-afea-b95cd93e9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f3b557-acc7-4543-b252-8f07d617dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-23 02:43:37.888946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15659 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:18:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape Word Ids :  (1, 128)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 128)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 128)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e7ed25-8d8f-47bc-b132-b017f72ad1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_dataset_from_tfds(in_memory_ds, info, split, batch_size,\n",
    "                           bert_preprocess_model):\n",
    "    is_training = split.startswith('train')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "  # dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    # iterator = iter(dataset)\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         _ = next(iterator)\n",
    "    #     except tf.errors.OutOfRangeError:\n",
    "    #         break\n",
    "    for _ in dataset:\n",
    "        pass\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "  \n",
    "    return dataset, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5434700e-4262-4e2c-8610-808d01e4aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d07116-ef38-42c2-baf0-ed6ae8154997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.2531782 0.3884658]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "text_classifier_model = build_classifier_model(2)\n",
    "bert_raw_result = text_classifier_model(text_preprocessed)\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9102f6b-2c5b-4791-9c14-f064442bb71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using glue/mrpc from TFDS\n",
      "This dataset has 5801 examples\n",
      "Number of classes: 2\n",
      "Features ['sentence1', 'sentence2']\n",
      "Splits ['train', 'validation', 'test']\n",
      "Here are some sample rows from glue/mrpc dataset\n",
      "['not_equivalent', 'equivalent']\n",
      "\n",
      "sample row 1\n",
      "b'The identical rovers will act as robotic geologists , searching for evidence of past water .'\n",
      "b'The rovers act as robotic geologists , moving on six wheels .'\n",
      "label: 0 (not_equivalent)\n",
      "\n",
      "sample row 2\n",
      "b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is completed .\"\n",
      "b\"Less than 20 percent of Boise 's sales would come from making lumber and paper after the OfficeMax purchase is complete , assuming those businesses aren 't sold .\"\n",
      "label: 0 (not_equivalent)\n",
      "\n",
      "sample row 3\n",
      "b'Spider-Man snatched $ 114.7 million in its debut last year and went on to capture $ 403.7 million .'\n",
      "b'Spider-Man , rated PG-13 , snatched $ 114.7 million in its first weekend and went on to take in $ 403.7 million .'\n",
      "label: 1 (equivalent)\n",
      "\n",
      "sample row 4\n",
      "b\"The 2002 second quarter results don 't include figures from our friends at Compaq .\"\n",
      "b'The year-ago numbers do not include figures from Compaq Computer .'\n",
      "label: 1 (equivalent)\n",
      "\n",
      "sample row 5\n",
      "b'Solomon 5.5 is available initially in the United States and Canada , for a starting price of about $ 12,700 .'\n",
      "b'Solomon 5.5 is now available in the U.S. and Canada through Microsoft Business Solutions resellers .'\n",
      "label: 0 (not_equivalent)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfds_name = 'glue/mrpc' \n",
    "\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'glue/mnli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "\n",
    "num_classes = tfds_info.features['label'].num_classes\n",
    "num_examples = tfds_info.splits.total_num_examples\n",
    "\n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'This dataset has {num_examples} examples')\n",
    "print(f'Number of classes: {num_classes}')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n",
    "\n",
    "# The code below is just to show some samples from the selected dataset\n",
    "print(f'Here are some sample rows from {tfds_name} dataset')\n",
    "sample_dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[train_split])\n",
    "\n",
    "labels_names = tfds_info.features['label'].names\n",
    "print(labels_names)\n",
    "print()\n",
    "\n",
    "sample_i = 1\n",
    "for sample_row in sample_dataset.take(5):\n",
    "  samples = [sample_row[feature] for feature in sentence_features]\n",
    "  print(f'sample row {sample_i}')\n",
    "  for sample in samples:\n",
    "    print(sample.numpy())\n",
    "  sample_label = sample_row['label']\n",
    "\n",
    "  print(f'label: {sample_label} ({labels_names[sample_label]})')\n",
    "  print()\n",
    "  sample_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc3489c-1874-4277-ac06-18338822bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  if glue_task == 'glue/mrpc':\n",
    "    metrics = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "  else:\n",
    "    metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        'accuracy', dtype=tf.float32)\n",
    "\n",
    "  return metrics, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b4df59c-162c-4a2f-921c-4b42820c597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['idx', 'label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    }
   ],
   "source": [
    "#loading datasets\n",
    "batch_size = 2\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "train_ds, train_ds_size = load_dataset_from_tfds(\n",
    "       in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "\n",
    "\n",
    "val_ds, val_ds_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "\n",
    "validation_steps = val_ds_size // batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d1e0271-739f-46e9-a18e-0f594714a380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loss function\n",
    "metrics, loss = get_configuration(tfds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6223dc3c-f0c8-47ab-a69a-fa523113daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create finetuning/optimizer\n",
    "epochs = 20\n",
    "steps_per_epoch = train_ds_size #batch size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = 10\n",
    "\n",
    "init_lr = 2e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bda4ea0-8edc-47df-b212-293ca0bf77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and train BERT\n",
    "classifier_model = build_classifier_model(num_classes)\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3e3febd-3572-4254-a687-490112590307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jcarbon.client import JCarbonClient\n",
    "from jcarbon.report import to_dataframe\n",
    "\n",
    "DEFAULT_PERIOD_MS = 10\n",
    "DEFAULT_SIGNALS = [\n",
    "    'jcarbon.cpu.eflect.ProcessEnergy',\n",
    "    'jcarbon.emissions.Emissions',\n",
    "    'jcarbon.server.MonotonicTimestamp',\n",
    "    'jcarbon.nvml.NvmlEstimatedEnergy',\n",
    "    'jcarbon.nvml.NvmlTotalEnergy',\n",
    "]\n",
    "\n",
    "class JCarbonEpochCallback2(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, addr='localhost:8980', period_ms=DEFAULT_PERIOD_MS, signals=DEFAULT_SIGNALS):\n",
    "        self.pid = os.getpid()\n",
    "        self.period_ms = period_ms\n",
    "        self.client = JCarbonClient(addr)\n",
    "        self.signals = signals\n",
    "        self.reports = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time = time.time()\n",
    "        self.chunks = []\n",
    "        self.client.start(self.pid, self.period_ms)\n",
    "\n",
    "    def on_train_batch_end(self, epoch, logs=None):\n",
    "        curr = time.time()\n",
    "        if(curr - self.time > 10):\n",
    "            self.client.stop(self.pid)\n",
    "            self.chunks.append(to_dataframe(\n",
    "                self.client.read(self.pid, self.signals)))\n",
    "            self.client.start(self.pid, self.period_ms)\n",
    "            self.time = curr\n",
    "            if logs is not None:\n",
    "                for (signal, component, unit), df in self.chunks[-1].groupby(['signal', 'component', 'unit']):\n",
    "                    # TODO: this should really not ignore negatives\n",
    "                    logs[f'jcarbon-epoch-{signal}-{component}-{unit}'] = df[df > 0].sum()\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.client.stop(self.pid)\n",
    "        self.chunks.append(to_dataframe(\n",
    "            self.client.read(self.pid, self.signals)))\n",
    "        self.reports.append(pd.concat(self.chunks))\n",
    "        if logs is not None:\n",
    "            for (signal, component, unit), df in self.reports[-1].groupby(['signal', 'component', 'unit']):\n",
    "                # TODO: this should really not ignore negatives\n",
    "                logs[f'jcarbon-epoch-{signal}-{component}-{unit}'] = df[df > 0].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931064b2-5edf-484c-8e74-c09210cd775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
      "Epoch 1/20\n",
      "  92/3668 [..............................] - ETA: 6:53 - loss: 0.8874 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1089 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1088 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 987.9644 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 986.5660"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 178/3668 [>.............................] - ETA: 6:45 - loss: 0.8784 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1062 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1062 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 963.5225 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 963.0580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 264/3668 [=>............................] - ETA: 6:36 - loss: 0.8898 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 956.1058 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 955.7087"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 350/3668 [=>............................] - ETA: 6:26 - loss: 0.8360 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 953.3066 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 953.0018"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 436/3668 [==>...........................] - ETA: 6:17 - loss: 0.8348 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1049 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 952.1695 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 951.9120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 522/3668 [===>..........................] - ETA: 6:07 - loss: 0.8491 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1049 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1049 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 951.9096 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 951.4967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 608/3668 [===>..........................] - ETA: 5:57 - loss: 0.8384 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1049 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 952.0582 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 951.9146"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 694/3668 [====>.........................] - ETA: 5:46 - loss: 0.8403 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 952.2399 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 952.2639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 781/3668 [=====>........................] - ETA: 5:36 - loss: 0.8339 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 953.1480 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 953.3916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 866/3668 [======>.......................] - ETA: 5:26 - loss: 0.8264 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1050 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 952.5417 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 952.7107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 952/3668 [======>.......................] - ETA: 5:16 - loss: 0.8178 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1051 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 953.0098 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 953.3016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039/3668 [=======>......................] - ETA: 5:06 - loss: 0.8204 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1052 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1052 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 954.1774 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 954.3101"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/3668 [========>.....................] - ETA: 4:56 - loss: 0.8230 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1053 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1053 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 954.7178 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 955.0785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213/3668 [========>.....................] - ETA: 4:45 - loss: 0.8203 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 955.8272 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 956.1306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/jcarbon/report.py:49: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  signals_df = pd.concat(signals_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297/3668 [=========>....................] - ETA: 4:35 - loss: 0.8220 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.1054 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 955.8272 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 956.1306"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "jcarb = JCarbonEpochCallback2(period_ms=4)\n",
    "\n",
    "classifier_model.fit(x=train_ds,\n",
    "                       validation_data=val_ds,\n",
    "                       epochs=epochs,\n",
    "                       steps_per_epoch=steps_per_epoch,\n",
    "                       validation_steps=val_ds_size,\n",
    "                       callbacks = jcarb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86fe4d94-ceb6-4b31-8ff2-15d9d97b846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start                          end                            ts            component                         unit          subcomponent              \n",
       "2024-05-23 06:44:26.826424598  2024-05-23 06:44:26.830587625  4.718526e+15  jcarbon.nvml.NvmlEstimatedEnergy  GRAMS_OF_CO2  device=0:name=Quadro P5000    0.000019\n",
       "                                                                            jcarbon.nvml.NvmlTotalEnergy      GRAMS_OF_CO2  device=0:name=Quadro P5000    0.000000\n",
       "2024-05-23 06:44:26.830587625  2024-05-23 06:44:26.834727764  4.718526e+15  jcarbon.nvml.NvmlEstimatedEnergy  GRAMS_OF_CO2  device=0:name=Quadro P5000    0.000019\n",
       "                                                                            jcarbon.nvml.NvmlTotalEnergy      GRAMS_OF_CO2  device=0:name=Quadro P5000    0.000000\n",
       "2024-05-23 06:44:26.832335189  2024-05-23 06:44:26.840588821  4.718526e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "                                                                                                                                                            ...   \n",
       "2024-05-23 06:51:40.142515225  2024-05-23 06:51:40.146590466  4.718956e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "2024-05-23 06:51:40.146590466  2024-05-23 06:51:40.150670200  4.718956e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "2024-05-23 06:51:40.150670200  2024-05-23 06:51:40.154744381  4.718956e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "2024-05-23 06:51:40.154744381  2024-05-23 06:51:40.158821690  4.718956e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "2024-05-23 06:51:40.158821690  2024-05-23 06:51:40.162896222  4.718956e+15  os=Linux                          GRAMS_OF_CO2  socket=0                      0.000000\n",
       "Name: value, Length: 261108, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jcarb.reports[0]['jcarbon.emissions.Emissions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ba33c-d8cd-4b45-9a56-53592d8afc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = 'jcarbon.emissions.Emissions'\n",
    "ax=None\n",
    "for report in jcarb.reports:\n",
    "    df = report.reset_index()\n",
    "    df = df[df.signal == signal]\n",
    "    df = df[df['value'] > 0]\n",
    "    df['elapsed'] = (df.start - df.start.min()).dt.total_seconds()\n",
    "    df['rolled_value'] = df['value'].rolling(1000).mean()\n",
    "    ax = df.plot(x='elapsed', y='rolled_value', xlabel = 'elapsed time', ylabel = 'emissions (grams of co2)' legend=False, figsize=(16, 9), ax=ax)\n",
    "ax.figure.savefig('rolled-plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64a318-652f-47bd-afd3-bf78a642ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = 'jcarbon.emissions.Emissions'\n",
    "x = 0\n",
    "df = pd.DataFrame()\n",
    "ax1 = None\n",
    "for report in jcarb.reports:\n",
    "    df_temp = report.reset_index()\n",
    "    df_temp = df_temp[df_temp.signal == signal]\n",
    "    df_temp = df_temp[df_temp['value'] > 0]\n",
    "    df_temp['epochs'] = x+1\n",
    "    df_temp['total_emissions'] = df_temp['value'].sum()\n",
    "    df = pd.concat([df_temp, df]) \n",
    "    x+=1\n",
    "ax1 = df.plot(x = 'epochs', y = 'total_emissions', xlabel = 'epoch', ylabel = 'emissions (grams of co2)', title = tfds_name)\n",
    "ax1.figure.savefig('epoch-to-co2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eed490-387b-4e03-b4b9-0421d54b44c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
