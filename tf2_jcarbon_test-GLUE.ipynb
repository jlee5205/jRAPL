{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc8ff9b-eb5d-4a4c-bcd1-1315c463b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 11:58:29.011019: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-25 11:58:29.063366: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 11:58:29.063403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 11:58:29.064940: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 11:58:29.073480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 11:58:30.084173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "from official.nlp import optimization\n",
    "from jcarbon.tensorflow.callbacks import JCarbonEpochCallback, JCarbonBatchCallback, JCarbonChunkingCallback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import time\n",
    "import sys\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6d55af-a7d4-4d70-96f7-02f1e3048586",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) != 3:\n",
    "    raise Exception(\"please enter dataset & output path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f77a92-b8fa-49ed-81e0-8508a3366c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#add bert model\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dd3a56-dbd4-40a3-afea-b95cd93e9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e7ed25-8d8f-47bc-b132-b017f72ad1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_dataset_from_tfds(in_memory_ds, info, split, batch_size,\n",
    "                           bert_preprocess_model):\n",
    "    is_training = split.startswith('train')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "  # dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    # iterator = iter(dataset)\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         _ = next(iterator)\n",
    "    #     except tf.errors.OutOfRangeError:\n",
    "    #         break\n",
    "    for _ in dataset:\n",
    "        pass\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "  \n",
    "    return dataset, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5434700e-4262-4e2c-8610-808d01e4aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc3489c-1874-4277-ac06-18338822bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  if glue_task == 'glue/cola':\n",
    "    metrics = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "  else:\n",
    "    metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        'accuracy', dtype=tf.float32)\n",
    "\n",
    "  return metrics, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f3b557-acc7-4543-b252-8f07d617dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 11:58:32.551932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15659 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:18:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape Word Ids :  (1, 128)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 128)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 128)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16c1721-5c0c-4942-9d80-7cc6c414e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_prefix = 'glue/'\n",
    "dataset = 'wnli'\n",
    "output_path = f'data/bert-2-128/{dataset}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0ba61c6-9815-4219-b4bd-54837075421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8748174 0.4873266]], shape=(1, 2), dtype=float32)\n",
      "Number of classes: 2\n",
      "This dataset has 852 examples\n",
      "Using glue/wnli from TFDS\n",
      "Features ['sentence1', 'sentence2']\n",
      "Splits ['train', 'validation', 'test']\n",
      "\n",
      "Fine tuning https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['idx', 'label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Epoch 1/10\n",
      "633/635 [============================>.] - ETA: 0s - loss: 0.7340 - accuracy: 0.5131 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 272.3568 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0384 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0383 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0350 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0300 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 348.1433 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 347.3293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 12:10:40.076263: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11569293893512737291\n",
      "2024-06-25 12:10:40.076309: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2220919409429503813\n",
      "2024-06-25 12:10:40.076320: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5084280170223247500\n",
      "2024-06-25 12:10:40.076326: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8411936789611678478\n",
      "2024-06-25 12:10:40.076334: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10169015479461164362\n",
      "2024-06-25 12:10:40.076340: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4256414430586712016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635/635 [==============================] - 19s 27ms/step - loss: 0.7340 - accuracy: 0.5129 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 455.9105 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0654 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0652 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0588 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0503 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 592.7933 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 591.4118 - val_loss: 0.8551 - val_accuracy: 0.2113\n",
      "Epoch 2/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.7121 - accuracy: 0.5370 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 361.9411 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0654 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0652 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0479 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0399 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 592.8223 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 591.1510\n",
      "Epoch 3/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.6905 - accuracy: 0.5641 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 368.8276 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0657 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0656 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0478 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0407 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 595.5354 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 595.3043\n",
      "Epoch 4/10\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 0.6778 - accuracy: 0.5799 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 362.4018 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0660 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0660 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0478 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0400 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 598.3619 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 598.4090\n",
      "Epoch 5/10\n",
      "635/635 [==============================] - 16s 26ms/step - loss: 0.6564 - accuracy: 0.6012 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 359.4417 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0663 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0662 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0478 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0396 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 601.1982 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 600.7620\n",
      "Epoch 6/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.6436 - accuracy: 0.6117 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 365.6035 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0667 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0666 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0480 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0403 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 604.7707 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 603.9998\n",
      "Epoch 7/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.6216 - accuracy: 0.6374 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 364.0701 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0679 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0678 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0480 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0401 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 616.2065 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 615.1240\n",
      "Epoch 8/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.6147 - accuracy: 0.6401 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 368.5213 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0684 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0683 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0480 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0406 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 620.6276 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 619.4967\n",
      "Epoch 9/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.6013 - accuracy: 0.6504 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 361.4650 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0686 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0686 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0480 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0398 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 621.9326 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 622.5710\n",
      "Epoch 10/10\n",
      "635/635 [==============================] - 17s 26ms/step - loss: 0.5933 - accuracy: 0.6546 - jcarbon-epoch-jcarbon.cpu.eflect.ProcessEnergy-process=53980-JOULES: 367.5544 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlEstimatedEnergy-GRAMS_OF_CO2: 0.0686 - jcarbon-epoch-jcarbon.emissions.Emissions-jcarbon.nvml.NvmlTotalEnergy-GRAMS_OF_CO2: 0.0685 - jcarbon-epoch-jcarbon.emissions.Emissions-os=Linux-GRAMS_OF_CO2: 0.0480 - jcarbon-epoch-jcarbon.emissions.Emissions-process=53980-GRAMS_OF_CO2: 0.0405 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 622.2787 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 621.5207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f3fec7c6d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classifier_model = build_classifier_model(2)\n",
    "bert_raw_result = text_classifier_model(text_preprocessed)\n",
    "print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "tfds_name = f'{tfds_prefix}{dataset}'\n",
    "tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "sentence_features = list(tfds_info.features.keys())\n",
    "sentence_features.remove('idx')\n",
    "sentence_features.remove('label')\n",
    "\n",
    "available_splits = list(tfds_info.splits.keys())\n",
    "train_split = 'train'\n",
    "validation_split = 'validation'\n",
    "test_split = 'test'\n",
    "if tfds_name == 'glue/mnli':\n",
    "  validation_split = 'validation_matched'\n",
    "  test_split = 'test_matched'\n",
    "    \n",
    "try:\n",
    "    num_classes = tfds_info.features['label'].num_classes\n",
    "    print(f'Number of classes: {num_classes}')\n",
    "except AttributeError:\n",
    "    print(\"num classes doesn't exist in {tfds_name}\")\n",
    "try:\n",
    "    num_examples = tfds_info.splits.total_num_examples\n",
    "    print(f'This dataset has {num_examples} examples')\n",
    "except AttributeError:\n",
    "    print(\"num examples doesn't exist in {tfds_name}\")\n",
    "\n",
    "    \n",
    "print(f'Using {tfds_name} from TFDS')\n",
    "print(f'Features {sentence_features}')\n",
    "print(f'Splits {available_splits}\\n')\n",
    "\n",
    "with tf.device('/job:localhost'):\n",
    "  # batch_size=-1 is a way to load the dataset into memory\n",
    "  in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n",
    "\n",
    "    #loading datasets\n",
    "batch_size = 32\n",
    "\n",
    "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "\n",
    "train_ds, train_ds_size = load_dataset_from_tfds(\n",
    "       in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "\n",
    "\n",
    "val_ds, val_ds_size = load_dataset_from_tfds(\n",
    "      in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "      bert_preprocess_model)\n",
    "\n",
    "validation_steps = val_ds_size // batch_size\n",
    "\n",
    "#create loss function\n",
    "metrics, loss = get_configuration(tfds_name)\n",
    "\n",
    "#create finetuning/optimizer\n",
    "epochs = 10\n",
    "steps_per_epoch = train_ds_size #batch size\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = 10\n",
    "\n",
    "init_lr = 2e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')\n",
    "#load and train BERT\n",
    "classifier_model = build_classifier_model(num_classes)\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)\n",
    "\n",
    "#do the actual training\n",
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "jcarb = JCarbonChunkingCallback(addr='localhost:8980', period_ms= 10, chunking_period_sec = 5)\n",
    "\n",
    "classifier_model.fit(x=train_ds,\n",
    "                       validation_data=val_ds,\n",
    "                       epochs=epochs,\n",
    "                       steps_per_epoch=steps_per_epoch,\n",
    "                       validation_steps=val_ds_size,\n",
    "                       callbacks = jcarb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9c934d0-8f6b-491b-8eca-c224661dc3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "for i, report in enumerate(jcarb.reports):\n",
    "    reports.append(report.to_frame().assign(epoch=i))\n",
    "reports = pd.concat(reports)\n",
    "\n",
    "output = f'/home/vincent/jRAPL/{output_path}/report_0.csv'\n",
    "reports.to_csv(output, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd010b68-f2ff-411d-b5ad-cbed9e47fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  signal                          start  \\\n",
      "0       jcarbon.cpu.eflect.ProcessEnergy  2024-06-25 16:10:20.873322083   \n",
      "1       jcarbon.cpu.eflect.ProcessEnergy  2024-06-25 16:10:20.874927752   \n",
      "2       jcarbon.cpu.eflect.ProcessEnergy  2024-06-25 16:10:20.883342013   \n",
      "3       jcarbon.cpu.eflect.ProcessEnergy  2024-06-25 16:10:20.884984595   \n",
      "4       jcarbon.cpu.eflect.ProcessEnergy  2024-06-25 16:10:20.893395633   \n",
      "...                                  ...                            ...   \n",
      "202555      jcarbon.nvml.NvmlTotalEnergy  2024-06-25 16:13:08.998927116   \n",
      "202556      jcarbon.nvml.NvmlTotalEnergy  2024-06-25 16:13:09.009119987   \n",
      "202557      jcarbon.nvml.NvmlTotalEnergy  2024-06-25 16:13:09.019315004   \n",
      "202558      jcarbon.nvml.NvmlTotalEnergy  2024-06-25 16:13:09.029505968   \n",
      "202559      jcarbon.nvml.NvmlTotalEnergy  2024-06-25 16:13:09.039632081   \n",
      "\n",
      "                                  end            ts      component    unit  \\\n",
      "0       2024-06-25 16:10:20.874927752  7.603677e+15  process=53980  JOULES   \n",
      "1       2024-06-25 16:10:20.883303625  7.603677e+15  process=53980  JOULES   \n",
      "2       2024-06-25 16:10:20.884984595  7.603677e+15  process=53980  JOULES   \n",
      "3       2024-06-25 16:10:20.893358131  7.603677e+15  process=53980  JOULES   \n",
      "4       2024-06-25 16:10:20.895041632  7.603677e+15  process=53980  JOULES   \n",
      "...                               ...           ...            ...     ...   \n",
      "202555  2024-06-25 16:13:09.009119987  7.603845e+15           nvml  JOULES   \n",
      "202556  2024-06-25 16:13:09.019315004  7.603845e+15           nvml  JOULES   \n",
      "202557  2024-06-25 16:13:09.029505968  7.603845e+15           nvml  JOULES   \n",
      "202558  2024-06-25 16:13:09.039632081  7.603845e+15           nvml  JOULES   \n",
      "202559  2024-06-25 16:13:09.049770593  7.603845e+15           nvml  JOULES   \n",
      "\n",
      "                      subcomponent     value  epoch  \n",
      "0         process=53980:task=53980  0.169560      0  \n",
      "1         process=53980:task=53980  0.884500      0  \n",
      "2         process=53980:task=53980  0.161206      0  \n",
      "3         process=53980:task=53980  0.821794      0  \n",
      "4         process=53980:task=53980  0.167461      0  \n",
      "...                            ...       ...    ...  \n",
      "202555  device=0:name=Quadro P5000  0.000000      9  \n",
      "202556  device=0:name=Quadro P5000  0.943000      9  \n",
      "202557  device=0:name=Quadro P5000  0.000000      9  \n",
      "202558  device=0:name=Quadro P5000  0.915000      9  \n",
      "202559  device=0:name=Quadro P5000  0.000000      9  \n",
      "\n",
      "[202560 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(output)\n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85648e62-9c3e-4dce-ba7a-d2808f30bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for report in jcarb.reports:\n",
    "#     df = report.reset_index()\n",
    "#     dfs.append(df)\n",
    "\n",
    "# concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "# concatenated_df.to_csv('data/report_yum.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879526be-13f2-4175-b64b-feaa3cddf0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
