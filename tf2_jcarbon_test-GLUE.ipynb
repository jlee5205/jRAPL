{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fc8ff9b-eb5d-4a4c-bcd1-1315c463b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:03:24.058377: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 00:03:24.111226: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 00:03:24.111267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 00:03:24.112822: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 00:03:24.120931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 00:03:25.162110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/vincent/.local/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text  # A dependency of the preprocessing model\n",
    "from official.nlp import optimization\n",
    "from jcarbon.tensorflow.callbacks import JCarbonEpochCallback, JCarbonBatchCallback, JCarbonChunkingCallback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import time\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f77a92-b8fa-49ed-81e0-8508a3366c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#add bert model\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n",
    "#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5dd3a56-dbd4-40a3-afea-b95cd93e9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
    "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
    "\n",
    "  Args:\n",
    "    sentence_features: a list with the names of string-valued features.\n",
    "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model that can be called on a list or dict of string Tensors\n",
    "    (with the order or names, resp., given by sentence_features) and\n",
    "    returns a dict of tensors for input to BERT.\n",
    "  \"\"\"\n",
    "\n",
    "  input_segments = [\n",
    "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
    "      for ft in sentence_features]\n",
    "\n",
    "  # Tokenize the text to word pieces.\n",
    "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
    "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
    "  segments = [tokenizer(s) for s in input_segments]\n",
    "\n",
    "  # Optional: Trim segments in a smart way to fit seq_length.\n",
    "  # Simple cases (like this example) can skip this step and let\n",
    "  # the next step apply a default truncation to approximately equal lengths.\n",
    "  truncated_segments = segments\n",
    "\n",
    "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
    "  # are model-dependent, so this gets loaded from the SavedModel.\n",
    "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
    "                          arguments=dict(seq_length=seq_length),\n",
    "                          name='packer')\n",
    "  model_inputs = packer(truncated_segments)\n",
    "  return tf.keras.Model(input_segments, model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e7ed25-8d8f-47bc-b132-b017f72ad1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_dataset_from_tfds(in_memory_ds, info, split, batch_size,\n",
    "                           bert_preprocess_model):\n",
    "    is_training = split.startswith('train')\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(in_memory_ds[split])\n",
    "    num_examples = info.splits[split].num_examples\n",
    "\n",
    "  # dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    # iterator = iter(dataset)\n",
    "    # while True:\n",
    "    #     try:\n",
    "    #         _ = next(iterator)\n",
    "    #     except tf.errors.OutOfRangeError:\n",
    "    #         break\n",
    "    for _ in dataset:\n",
    "        pass\n",
    "    dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
    "  \n",
    "    return dataset, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5434700e-4262-4e2c-8610-808d01e4aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(num_classes):\n",
    "\n",
    "  class Classifier(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "      super(Classifier, self).__init__(name=\"prediction\")\n",
    "      self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "      self.dense = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, preprocessed_text):\n",
    "      encoder_outputs = self.encoder(preprocessed_text)\n",
    "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
    "      x = self.dropout(pooled_output)\n",
    "      x = self.dense(x)\n",
    "      return x\n",
    "\n",
    "  model = Classifier(num_classes)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc3489c-1874-4277-ac06-18338822bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_configuration(glue_task):\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "  if glue_task == 'glue/cola':\n",
    "    metrics = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "  else:\n",
    "    metrics = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "        'accuracy', dtype=tf.float32)\n",
    "\n",
    "  return metrics, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6f3b557-acc7-4543-b252-8f07d617dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:03:27.885284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15659 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:18:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys           :  ['input_mask', 'input_word_ids', 'input_type_ids']\n",
      "Shape Word Ids :  (1, 128)\n",
      "Word Ids       :  tf.Tensor(\n",
      "[ 101 2070 6721 3231 6251  102 2178 6251  102    0    0    0    0    0\n",
      "    0    0], shape=(16,), dtype=int32)\n",
      "Shape Mask     :  (1, 128)\n",
      "Input Mask     :  tf.Tensor([1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n",
      "Shape Type Ids :  (1, 128)\n",
      "Type Ids       :  tf.Tensor([0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0], shape=(16,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "test_preprocess_model = make_bert_preprocess_model(['my_input1', 'my_input2'])\n",
    "test_text = [np.array(['some random test sentence']),\n",
    "             np.array(['another sentence'])]\n",
    "text_preprocessed = test_preprocess_model(test_text)\n",
    "\n",
    "print('Keys           : ', list(text_preprocessed.keys()))\n",
    "print('Shape Word Ids : ', text_preprocessed['input_word_ids'].shape)\n",
    "print('Word Ids       : ', text_preprocessed['input_word_ids'][0, :16])\n",
    "print('Shape Mask     : ', text_preprocessed['input_mask'].shape)\n",
    "print('Input Mask     : ', text_preprocessed['input_mask'][0, :16])\n",
    "print('Shape Type Ids : ', text_preprocessed['input_type_ids'].shape)\n",
    "print('Type Ids       : ', text_preprocessed['input_type_ids'][0, :16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e16c1721-5c0c-4942-9d80-7cc6c414e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = ['mrpc', 'cola', 'sst2', 'qqp', 'stsb', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "#index = ['wnli', 'rte', 'mrpc', 'cola', 'sst2','qnli','qqp','mnli','sst2']\n",
    "index = ['cola','mrpc','wnli']\n",
    "total_cpu = []\n",
    "total_gpu = []\n",
    "tfds_prefix = 'glue/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66eda7f7-9b34-429b-bfbc-1582e323bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart_plot(jcarb):\n",
    "    signal = 'jcarbon.emissions.Emissions'\n",
    "    ax = None\n",
    "    x = 0\n",
    "    df1 = pd.DataFrame({'cpu_emissions':[], 'gpu_emissions':[], 'epochs':[]});\n",
    "    for report in jcarb.reports:\n",
    "        x+=1\n",
    "        df = report.reset_index()\n",
    "        df = df[df.signal == signal]\n",
    "        df = df[df['value'] >= 0]\n",
    "        df['cpu'] = df.subcomponent\n",
    "        groups = df.groupby('cpu')\n",
    "        cpu_df = groups.get_group('socket=0')\n",
    "        gpu_df = groups.get_group('device=0:name=Quadro P5000')\n",
    "        new_row = pd.DataFrame({'cpu_emissions': [cpu_df['value'].sum()], 'gpu_emissions':[gpu_df['value'].sum()], 'epochs':[x]});\n",
    "        df1 = pd.concat([new_row,df1])\n",
    "    cpu_sum = df1['cpu_emissions'].sum()\n",
    "    gpu_sum = df1['gpu_emissions'].sum()\n",
    "    return cpu_sum, gpu_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b912f229-d0bf-4b7e-8a88-3cf42a9b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacked line plot for cpu + gpu\n",
    "def stacked_line_plot(jcarb, dataset):\n",
    "    signal = 'jcarbon.emissions.Emissions'\n",
    "    ax = None\n",
    "    x = 0\n",
    "    df1 = pd.DataFrame({'cpu_emissions':[], 'gpu_emissions':[], 'epochs':[]});\n",
    "    for report in jcarb.reports:\n",
    "        x+=1\n",
    "        df = report.reset_index()\n",
    "        df = df[df.signal == signal]\n",
    "        df = df[df['value'] >= 0]\n",
    "        df['cpu'] = df.subcomponent\n",
    "        groups = df.groupby('cpu')\n",
    "        cpu_df = groups.get_group('socket=0')\n",
    "        gpu_df = groups.get_group('device=0:name=Quadro P5000')\n",
    "        new_row = pd.DataFrame({'cpu_emissions': [cpu_df['value'].sum()], 'gpu_emissions':[gpu_df['value'].sum()], 'epochs':[x]});\n",
    "        df1 = pd.concat([new_row,df1])\n",
    "    params = bert_model_name.strip('small_bert/bert_en_uncased_')\n",
    "    path_name = f'tf_plots/line_plots/{dataset}-plot.png'\n",
    "    raw_path = f'tf_plots/line_plots/raw_data/{dataset}.csv'\n",
    "    df1.to_csv(raw_path, index = False)\n",
    "    ax = df1.plot(x = 'epochs', stacked = True, xlabel = 'epochs', ylabel = 'emissions (grams of co2)', title = f'{tfds_name} with {params}') \n",
    "    ax.locator_params(integer=True)\n",
    "    ax.figure.savefig(path_name)\n",
    "# ax1 = df.plot(x = 'epochs', y = 'total_emissions', xlabel = 'epoch', ylabel = 'emissions (grams of co2)', title = tfds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f44c110-49f1-4f97-8fd1-d7664d862fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time series plot\n",
    "def time_series_plot(jcarb, dataset):\n",
    "    signal = 'jcarbon.emissions.Emissions'\n",
    "    ax=None\n",
    "    for report in jcarb.reports:\n",
    "        df = report.reset_index()\n",
    "        df = df[df.signal == signal]\n",
    "        df = df[df['value'] >= 0]\n",
    "        df['elapsed'] = (df.start - df.start.min()).dt.total_seconds()\n",
    "        df['rolled_value'] = df['value'].rolling(1000).mean()\n",
    "        ax = df.plot(x='elapsed', y='rolled_value', xlabel = 'elapsed time', ylabel = 'emissions (grams of co2)', legend=False, figsize=(16, 9), ax=ax)\n",
    "    params = bert_model_name.strip('small_bert/bert_en_uncased_')\n",
    "    path_name = f'tf_plots/time_series/{dataset}-plot-series.png'\n",
    "    raw_path = f'tf_plots/time_series/raw_data/{dataset}.csv'\n",
    "    df.to_csv(raw_path, index = False)\n",
    "    ax.figure.savefig(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b94d323-6f3b-4841-b2dd-c3cc5f691d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = ['jcarbon.emissions.Emissions', 'jcarbon.server.MonotonicTimestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670ec71b-f9d7-474f-988b-18446c2ae83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_timestamps(timestamps, bucket_size_ms):\n",
    "    \"\"\" normalizes ns timestamps to ms-bucketed timestamps \"\"\"\n",
    "    # TODO: this is producing strange behavior due to int division:\n",
    "    #   2938450289096200 // 10**6 = 2938450288\n",
    "    # TODO: taken from vesta's source. need to determine how to merge\n",
    "    return bucket_size_ms * (timestamps // 10**6 // bucket_size_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f733ce33-4265-4855-ba43-6b1146bfff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe(report, signals=None):\n",
    "    signals_df = []\n",
    "    monotonic_time = {}\n",
    "    for jcarbon_signal in report.signal:\n",
    "        if jcarbon_signal.signal_name == 'jcarbon.server.MonotonicTimestamp':\n",
    "            # TODO: for now, i'm always grabbing the monotonic time.\n",
    "            monotonic_time = {}\n",
    "            for signal in jcarbon_signal.signal:\n",
    "                start = 1000000000 * signal.start.secs + signal.start.nanos\n",
    "                for data in signal.data:\n",
    "                    monotonic_time[start] = data.value\n",
    "            monotonic_time = pd.Series(monotonic_time)\n",
    "            monotonic_time.index.name = 'timestamp'\n",
    "            monotonic_time.name = 'ts'\n",
    "        elif signals is None or jcarbon_signal.signal_name in signals:\n",
    "            df = []\n",
    "            for signal in jcarbon_signal.signal:\n",
    "                start = 1000000000 * signal.start.secs + signal.start.nanos\n",
    "                end = 1000000000 * signal.end.secs + signal.end.nanos\n",
    "                for data in signal.data:\n",
    "                    df.append([\n",
    "                        jcarbon_signal.signal_name,\n",
    "                        start,\n",
    "                        end,\n",
    "                        signal.component.replace(',', ':'),\n",
    "                        signal.unit,\n",
    "                        data.component.replace(',', ':'),\n",
    "                        data.value,\n",
    "                    ])\n",
    "            signals_df.append(pd.DataFrame(data=df, columns=[\n",
    "                              'signal', 'start', 'end', 'component', 'unit', 'subcomponent', 'value']))\n",
    "\n",
    "    signals_df = pd.concat(signals_df)\n",
    "    diff = (signals_df.end - signals_df.start).min() // 1000\n",
    "    signals_df['start_norm'] = normalize_timestamps(signals_df.start, diff)\n",
    "    # monotonic_time.index = normalize_timestamps(monotonic_time.index, diff)\n",
    "    signals_df['ts'] = 0\n",
    "    signals_df['start'] = pd.to_datetime(signals_df.start, unit='ns')\n",
    "    signals_df['end'] = pd.to_datetime(signals_df.end, unit='ns')\n",
    "\n",
    "    return signals_df.set_index(['signal', 'start', 'end', 'ts', 'component', 'unit', 'subcomponent']).value.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e95fbb9b-1a32-443c-a53e-1538cad01609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from jcarbon.client import JCarbonClient\n",
    "# from jcarbon.report import to_dataframe\n",
    "\n",
    "DEFAULT_PERIOD_MS = 10\n",
    "DEFAULT_SIGNALS = [\n",
    "    'jcarbon.cpu.eflect.ProcessEnergy',\n",
    "    'jcarbon.emissions.Emissions',\n",
    "    'jcarbon.server.MonotonicTimestamp',\n",
    "    'jcarbon.nvml.NvmlEstimatedEnergy',\n",
    "    'jcarbon.nvml.NvmlTotalEnergy',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b8b76b-0cfc-4b32-8680-2c9aa7b20fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JCarbonChunkingCallback(Callback):\n",
    "    def __init__(self, addr='localhost:8980', period_ms=DEFAULT_PERIOD_MS, signals=DEFAULT_SIGNALS):\n",
    "        self.pid = os.getpid()\n",
    "        self.period_ms = period_ms\n",
    "        self.client = JCarbonClient(addr)\n",
    "        self.signals = signals\n",
    "        self.reports = []\n",
    "        print(\"hello\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.time = time.time()\n",
    "        self.chunks = []\n",
    "        self.client.start(self.pid, self.period_ms)\n",
    "\n",
    "    def on_train_batch_end(self, epoch, logs=None):\n",
    "        curr = time.time()\n",
    "        if(curr - self.time > 10):\n",
    "            self.client.stop(self.pid)\n",
    "            self.chunks.append(to_dataframe(\n",
    "                self.client.read(self.pid, self.signals)))\n",
    "            self.client.start(self.pid, self.period_ms)\n",
    "            self.time = curr\n",
    "            if logs is not None:\n",
    "                for (signal, component, unit), df in self.chunks[-1].groupby(['signal', 'component', 'unit']):\n",
    "                    # TODO: this should really not ignore negatives\n",
    "                    logs[f'jcarbon-epoch-{signal}-{component}-{unit}'] = df[df > 0].sum()\n",
    "                \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.client.stop(self.pid)\n",
    "        self.chunks.append(to_dataframe(\n",
    "            self.client.read(self.pid, self.signals)))\n",
    "        self.reports.append(pd.concat(self.chunks))\n",
    "        if logs is not None:\n",
    "            for (signal, component, unit), df in self.reports[-1].groupby(['signal', 'component', 'unit']):\n",
    "                # TODO: this should really not ignore negatives\n",
    "                logs[f'jcarbon-epoch-{signal}-{component}-{unit}'] = df[df > 0].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba61c6-9815-4219-b4bd-54837075421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.29277614 0.25735623]], shape=(1, 2), dtype=float32)\n",
      "Number of classes: 2\n",
      "This dataset has 10657 examples\n",
      "Using glue/cola from TFDS\n",
      "Features ['sentence']\n",
      "Splits ['train', 'validation', 'test']\n",
      "\n",
      "Fine tuning https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.9/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['idx', 'label'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "hello\n",
      "Epoch 1/100\n",
      "8550/8551 [============================>.] - ETA: 0s - loss: 0.5256 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 757.4418 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 786.3670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 00:07:31.725581: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16812416964289065126\n",
      "2024-06-12 00:07:31.725669: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16438108324085350556\n",
      "2024-06-12 00:07:31.725687: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10313258542156104508\n",
      "2024-06-12 00:07:31.725702: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6265514883361153838\n",
      "2024-06-12 00:07:31.725716: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6340696860084209936\n",
      "2024-06-12 00:07:31.725730: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8716960273430886980\n",
      "2024-06-12 00:07:31.725748: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13737590014512061598\n",
      "2024-06-12 00:07:31.725762: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11919467258475597348\n",
      "2024-06-12 00:07:31.725786: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1122604032529510705\n",
      "2024-06-12 00:07:31.725888: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9474390625632744433\n",
      "2024-06-12 00:07:31.725908: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9525136198866334789\n",
      "2024-06-12 00:07:31.725924: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16070262993181693337\n",
      "2024-06-12 00:07:31.725938: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5472993558670514073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8551/8551 [==============================] - 231s 26ms/step - loss: 0.5256 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 1479.9150 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 1536.2424 - val_loss: 0.8142 - val_MatthewsCorrelationCoefficient: 0.0000e+00\n",
      "Epoch 2/100\n",
      "8551/8551 [==============================] - 217s 25ms/step - loss: 0.3247 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 1395.6074 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 1448.1011\n",
      "Epoch 3/100\n",
      "6342/8551 [=====================>........] - ETA: 55s - loss: 0.2305 - MatthewsCorrelationCoefficient: 0.0000e+00 - jcarbon-epoch-jcarbon.nvml.NvmlEstimatedEnergy-nvml-JOULES: 722.7337 - jcarbon-epoch-jcarbon.nvml.NvmlTotalEnergy-nvml-JOULES: 750.9195"
     ]
    }
   ],
   "source": [
    "for dataset in index:\n",
    "    text_classifier_model = build_classifier_model(2)\n",
    "    bert_raw_result = text_classifier_model(text_preprocessed)\n",
    "    print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "    tfds_name = f'{tfds_prefix}{dataset}'\n",
    "    tfds_info = tfds.builder(tfds_name).info\n",
    "\n",
    "    sentence_features = list(tfds_info.features.keys())\n",
    "    sentence_features.remove('idx')\n",
    "    sentence_features.remove('label')\n",
    "    \n",
    "    available_splits = list(tfds_info.splits.keys())\n",
    "    train_split = 'train'\n",
    "    validation_split = 'validation'\n",
    "    test_split = 'test'\n",
    "    if tfds_name == 'glue/mnli':\n",
    "      validation_split = 'validation_matched'\n",
    "      test_split = 'test_matched'\n",
    "        \n",
    "    try:\n",
    "        num_classes = tfds_info.features['label'].num_classes\n",
    "        print(f'Number of classes: {num_classes}')\n",
    "    except AttributeError:\n",
    "        print(\"num classes doesn't exist in {tfds_name}\")\n",
    "    try:\n",
    "        num_examples = tfds_info.splits.total_num_examples\n",
    "        print(f'This dataset has {num_examples} examples')\n",
    "    except AttributeError:\n",
    "        print(\"num examples doesn't exist in {tfds_name}\")\n",
    "    \n",
    "        \n",
    "    print(f'Using {tfds_name} from TFDS')\n",
    "    print(f'Features {sentence_features}')\n",
    "    print(f'Splits {available_splits}\\n')\n",
    "    \n",
    "    with tf.device('/job:localhost'):\n",
    "      # batch_size=-1 is a way to load the dataset into memory\n",
    "      in_memory_ds = tfds.load(tfds_name, batch_size=-1, shuffle_files=True)\n",
    "\n",
    "        #loading datasets\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f'Fine tuning {tfhub_handle_encoder} model')\n",
    "    bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
    "    \n",
    "    train_ds, train_ds_size = load_dataset_from_tfds(\n",
    "           in_memory_ds, tfds_info, train_split, batch_size, bert_preprocess_model)\n",
    "    \n",
    "    \n",
    "    val_ds, val_ds_size = load_dataset_from_tfds(\n",
    "          in_memory_ds, tfds_info, validation_split, batch_size,\n",
    "          bert_preprocess_model)\n",
    "    \n",
    "    validation_steps = val_ds_size // batch_size\n",
    "\n",
    "    #create loss function\n",
    "    metrics, loss = get_configuration(tfds_name)\n",
    "    \n",
    "    #create finetuning/optimizer\n",
    "    epochs = 100\n",
    "    steps_per_epoch = train_ds_size #batch size\n",
    "    num_train_steps = steps_per_epoch * epochs\n",
    "    num_warmup_steps = 10\n",
    "    \n",
    "    init_lr = 2e-5\n",
    "    optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                              num_train_steps=num_train_steps,\n",
    "                                              num_warmup_steps=num_warmup_steps,\n",
    "                                              optimizer_type='adamw')\n",
    "    #load and train BERT\n",
    "    classifier_model = build_classifier_model(num_classes)\n",
    "    classifier_model.compile(optimizer=optimizer,\n",
    "                             loss=loss,\n",
    "                             metrics=metrics)\n",
    "\n",
    "    #do the actual training\n",
    "    print(f'Training model with {tfhub_handle_encoder}')\n",
    "    jcarb = JCarbonChunkingCallback(addr='localhost:8981', period_ms=1000, signals=signal)\n",
    "    \n",
    "    classifier_model.fit(x=train_ds,\n",
    "                           validation_data=val_ds,\n",
    "                           epochs=epochs,\n",
    "                           steps_per_epoch=steps_per_epoch,\n",
    "                           validation_steps=val_ds_size,\n",
    "                           callbacks = jcarb)\n",
    "    cpu_emissions, gpu_emissions = bar_chart_plot(jcarb)\n",
    "    stacked_line_plot(jcarb, dataset)\n",
    "    time_series_plot(jcarb, dataset)\n",
    "    \n",
    "    total_cpu.append(cpu_emissions)\n",
    "    total_gpu.append(gpu_emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d07116-ef38-42c2-baf0-ed6ae8154997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'total_cpu_emissions': total_cpu,\n",
    "                   'total_gpu_emissions': total_gpu,\n",
    "                   'dataset':index}, index=index)\n",
    "ax = df.plot.bar(stacked = True, ylabel = 'emissions (grams of co2)')\n",
    "ax.figure.savefig('tf_plots/bar_chart.png')\n",
    "raw_path = f'tf_plots/plot_data.csv'\n",
    "df.to_csv(raw_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eed490-387b-4e03-b4b9-0421d54b44c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8880840-c9f9-4ccc-bbcf-c411700b1ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
